# Lesson 1

I'm hopeful about the formatting of this course. I think a top-down approach to this will be useeful, because I haven't the math chops to keep up with a bottom-up approach yet. Learning the architecture will help give me an intuition behind the more complex math when I get to that part.

- I need to follow the instructions more carefully. Generally, when I'm having a problem, it's because I didn't follow instructions to the T.

One of the biggest things I picked up in this lesson was the part about neural networks being able to solve any solvable problem with two layers. This boggles my mind; how can only two layers of artificial neurons have the complexity to solve such a wide range of problems? (Side question: does it matter the *length* of each layer?) Naturally, it all comes down to weights. With the correct weighting (parameters), neural networks can solve a vast range of problems.

One other big thing I thought about was that neural networks seem to literally be how we think. How is it that we find it simple, even childish, to classify a dog from a cat, while trying to program this into a computer can take years (insert XKCD comic here)? I think neural networks hits the problem on the mark: we don't look for features persay, rather our brains unconsciously abstract features for us, assigning them weights, until it is natural and automatic for us to differentiate between a cat and a dog. As a toddler, everything with four legs may be a dog (or a cat, depending on which word you learned first). As you point to a dog, you might think it to be a cat; after all, it is furry, has 4 legs, pointy ears, and cute paws. 
![cat-dog](https://user-images.githubusercontent.com/83550862/186196606-ed4ba834-db57-49c4-89e2-d7f7bab3fc31.jpg)
However, your prediction was wrong, and you (hopefully) get swift feedback from a better-trained algorithm (perhaps a parent or sibling) informing you of your error. This helps you adjust your parameters for the next time you encounter a furry animal, and in time you become quite an adept cat-recognizer.

I thought the idea of allowing the computer to abstract features for itself is the only way to go. How did we ever think that hard-coding recognition algorithms was the most efficient way to do anything? Arthur Samuel is quite a man, I'm glad he introduced this idea to the general AI community.

We have lots of hardware to augment what humans are bad at. We are bad at remembering things, complex mathematical calculation, and staying awake. We have solved lots of these problems, and are now moving on to the problem of automating the things we're already good at: facial recognition, making quick decisions given disparate data, and in general, combining different parameters to come to a decision. One example I think of a lot are the chicken sexers of Japan. They can sex a chicken in less than a seecond, and not have any clue what specifically led them to that gender. Rather, a general pattern on the backside of the chicken gives "male vibes" or "female vibes". What's really happening is their brain has created a model for chicken genders, assigning weights to the different spot placements on a chicken. I think neural networks, from what I've seen so far, emulate the human brain the best out of most machine learning models, and thus are best suited currently to help automate these cognitive tasks. I think we're on the brink of an explosion of human automation, automating the rote but complex tasks that humans must perform.

