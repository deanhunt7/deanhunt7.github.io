http://neuralnetworksanddeeplearning.com/chap4.html

This website gave an AWESOME visual proof of the Universal Approximation Theorem! In this blog post, I'm going to discuss my understandings and any questions/branches I've made.

I was amazed that a single hidden layer could model any given problem in the world. I mean, there's no way, right? What I didn't realize were two key things:

1. The assumption that all problems to be solved in the world can be reduced to a function.
This function may be in R<sup>100</sup>, but it's still a function. I didn't make this connection, and I think it's a fundamental change in the way I usually think. I kind of compare it to what I think about music being an equation.

2. The models that they are talking about, to approximate any reasonable complex function, are going to be unreasonably large.
Deep learning isn't the cure to all of the world's problems, at least not yet. It's like saying that any algorithm is able to be cracked. Well, sure, given a billion years. We are talking about human scales here, and on these scales these models are generally way too large to be feasible.

After learning these, I still think the potential of neural nets is amazing. I can draw so many parallels between the operation of neural nets, and the operation of our human brain. It's amazing to think that these artificial neurons exist as acutal biological systems inside of all of our heads. That's worth thinking about for a long time, I think. I believe modeling an architecture to think and learn like humans is one of the most important things I can be doing right now, beacuse I think this is the future of automation. We've pretty much solved the automation of purely robotic labor, but have swaths of labor requiring at least minimal cognition. These are the most sinister problems to solve, because while they may seem simple to a pre-trained algorithm (in the form of a human), a computer would have an awful time with it because of their inflexibility. Neural nets propose to solve this problem, and I'm super excited to see it happen.

One area where I think this is especially relevant is in computer science, of all places. If you've ever made a basic website, I'm sure you've encountered the numerous schleps along the way, feeling like a moron while trying to center a div. I definitely liken this kind of work to manual labor, albiet a different form than the term "manual labor" generally refers to. With the plasticity that neural nets offer, I think that the automation of this kind of programming is in the near future. Imagine not having to spend hours creating a website, but rather spending that time on truly creative endeavors, such as designing a product to be sold on that website. I think this would be infinitely better than the current setup.

The gist of sigmoid functions is they give granularity to activations. Rather than being a simple on/off, like a perceptron, they allow a sort of spectrum of values, and this granularity can help devise more accurate parameters to feed in. However, assign a large enough weight to a sigmoid neuron, and they behave indistinguishably from a perceptron. 

Given a large amount of neurons in the hidden layer, you can then use these step functions to get close to your approximation of the curve.

![image](https://user-images.githubusercontent.com/83550862/187093238-b7120427-8f73-440d-8928-53adbc414f17.png)

As n -> infinity, the model will *exactly* follow the function (for the given range). Obviously we can't have an infinite number of neurons, but we can approximate it well enough that it tends not to matter for most applications of neural nets.

This extends for models with multiple inputs, as well. Rather than picturing a 2-D graph, it's easy to envision the surface of your function you want to approximate.

![image](https://user-images.githubusercontent.com/83550862/187093315-3396332f-44eb-44e9-82d0-0eec5b3dd33e.png)

By creating these "towers" using the same method as that of creating the "perceptron" spikes in the single-input version, you can effectively model a function in R<sup>3</sup>.
